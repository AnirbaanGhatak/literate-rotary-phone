{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv('Train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      I grew up (b. 1965) watching and loving the Th...\n",
       "1      When I put this movie in my DVD player, and sa...\n",
       "2      Why do people who do not know what a particula...\n",
       "3      Even though I have great interest in Biblical ...\n",
       "4      Im a die hard Dads Army fan and nothing will e...\n",
       "...                                                  ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...\n",
       "39996  This movie is an incredible piece of work. It ...\n",
       "39997  My wife and I watched this movie because we pl...\n",
       "39998  When I first watched Flatliners, I was amazed....\n",
       "39999  Why would this film be so good, but only gross...\n",
       "\n",
       "[40000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(row):\n",
    "# Convert to lowercase\n",
    "    text = row.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>[grew, b, watching, loving, thunderbirds, mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>[put, movie, dvd, player, sat, coke, chips, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>[people, know, particular, time, past, like, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>[even, though, great, interest, biblical, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>[im, die, hard, dads, army, fan, nothing, ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I grew up (b. 1965) watching and loving the Th...   \n",
       "1  When I put this movie in my DVD player, and sa...   \n",
       "2  Why do people who do not know what a particula...   \n",
       "3  Even though I have great interest in Biblical ...   \n",
       "4  Im a die hard Dads Army fan and nothing will e...   \n",
       "\n",
       "                                     tokenized_sents  \n",
       "0  [grew, b, watching, loving, thunderbirds, mate...  \n",
       "1  [put, movie, dvd, player, sat, coke, chips, ex...  \n",
       "2  [people, know, particular, time, past, like, f...  \n",
       "3  [even, though, great, interest, biblical, movi...  \n",
       "4  [im, die, hard, dads, army, fan, nothing, ever...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df['tokenized_sents'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text\n",
    "input_text = input(\"Enter your query: \")\n",
    "# Tokenized sentences from your dataset\n",
    "tokenized_sentences = df['tokenized_sents'].tolist()\n",
    "# Convert tokenized sentences into strings\n",
    "tokenized_sentences = [\" \".join(tokens) for tokens in tokenized_sentences]\n",
    "# Convert input text into a string if it's a list\n",
    "if isinstance(input_text, list):\n",
    "    input_text = \" \".join(input_text)\n",
    "# Convert tokenized sentences and input text into numerical vectors using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tokenized_sentences + [input_text])\n",
    "# Compute cosine similarity between input text vector and each vector in the dataset\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar products for Bags:\n",
      "                                               Title  Cosine Similarity\n",
      "0  I rented this because I'm a bit weary of '80s ...           0.187295\n",
      "1  I went in not knowing anything about this movi...           0.181219\n",
      "2  I was fascinated as to how truly bad this movi...           0.173942\n",
      "3  OK, I am a sucker. I loved it. I had no expect...           0.156673\n",
      "4  If you're a layman interested in quantum theor...           0.155651\n"
     ]
    }
   ],
   "source": [
    "# Get indices of most similar items\n",
    "most_similar_indices = cosine_similarities.argsort()[0][::-1][:5] # Get top 5\n",
    "#indices\n",
    "# Create a DataFrame to store top 5 similar products and their cosine\n",
    "#similarities\n",
    "top_similar_df = pd.DataFrame(columns=['Title', 'Cosine Similarity'])\n",
    "# Fill the DataFrame with data\n",
    "for idx in most_similar_indices:\n",
    "    text = df.loc[idx, 'text']\n",
    "    similarity = cosine_similarities[0][idx]\n",
    "    top_similar_df.loc[len(top_similar_df)] = [text, similarity]\n",
    "# Display the DataFrame\n",
    "print(f\"\\nTop 5 similar products for {input_text}:\")\n",
    "print(top_similar_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model = Word2Vec(sentences=df['tokenized_sents'], vector_size=100, window=5,\n",
    "min_count=1, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12780282  0.9071737  -0.2354934  -0.5303825   1.1827406   0.5659059\n",
      "  0.2964308   0.917051   -0.15370122  0.7271319  -0.9855236  -0.26626933\n",
      " -0.00430388  0.08617382  0.88665324 -0.79872483  0.08847836  0.29003546\n",
      " -0.44187838 -0.8943349   0.34208918  0.8010406   0.44939512 -0.09659745\n",
      "  0.04627722  0.0173268  -0.73725015 -0.52461576 -0.14174932 -0.7197496\n",
      "  0.19815166  0.08127058 -0.57616323  0.38616112  0.19289108  0.56673926\n",
      " -0.27318165 -0.4319313   0.5680353   0.25661844  0.4562481  -0.49407065\n",
      " -0.3386588   0.27295756 -0.1279482  -0.08139388 -0.27554268 -0.20281184\n",
      "  0.22055537 -0.8774142   0.6561442  -0.66929215 -0.10679027  0.18666504\n",
      " -0.547993    0.7109107   0.9624165  -0.51227903 -0.03076768 -0.59558177\n",
      " -0.22885568  0.56230384  0.17289296  0.01146731 -0.99664795  0.4141445\n",
      " -0.1369941  -0.3166605  -0.36577192  0.26997894 -0.36139053 -0.09298746\n",
      "  0.6922673   0.6842534   0.11200828 -0.10805236  0.59492356 -0.6782107\n",
      " -0.58614314 -0.05911744 -0.63365024 -0.09294061 -0.08500376  0.3228215\n",
      " -0.07520578 -0.40031376 -0.3860974   0.37612063 -0.6171055   0.01311606\n",
      "  0.72622544  0.72763646  1.0926684  -0.18777038  0.8508032  -0.45972353\n",
      "  0.2741831  -0.59653926  0.12363417  0.13258997]\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv['bag']\n",
    "print(vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('launchers', 0.9012715220451355)\n",
      "('grenades', 0.8995427489280701)\n",
      "('rifles', 0.895071268081665)\n",
      "('lamp', 0.895065188407898)\n",
      "('steel', 0.8931792378425598)\n",
      "('restaurants', 0.8930208086967468)\n",
      "('sewer', 0.8914008736610413)\n",
      "('rockets', 0.8899285197257996)\n",
      "('confiscated', 0.8865554332733154)\n",
      "('laboratory', 0.8859495520591736)\n",
      "('controls', 0.8857496976852417)\n",
      "('storage', 0.8854281902313232)\n",
      "('squatters', 0.8835244178771973)\n",
      "('cart', 0.8833296298980713)\n",
      "('installing', 0.8817546963691711)\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('mobile', topn=15)\n",
    "for i in similar_words:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
